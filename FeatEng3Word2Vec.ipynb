{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your own word2vec representations, as you did in the first example in this checkpoint. However, you need to experiment with the hyperparameters of the vectorization step. Modify the hyperparameters and run the classification models again. Can you wrangle any improvements?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation that spaCy doesn't\n",
    "    # recognize: the double dash --. Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean the data\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# the chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group into sentences\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# combine the sentences from the two novels into one data frame\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid off stop words and punctuation\n",
    "# and lemmatize the tokens\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec on the the sentences\n",
    "model1 = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=4,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")\n",
    "\n",
    "model2 = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=6,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")\n",
    "\n",
    "model3 = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=8,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")\n",
    "\n",
    "model4 = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=4,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=200,\n",
    "    hs=1\n",
    ")\n",
    "\n",
    "model5 = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=6,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=200,\n",
    "    hs=1\n",
    ")\n",
    "\n",
    "model6 = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=8,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=200,\n",
    "    hs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is going on here?\n",
    "\n",
    "word2vec_arr1 = np.zeros((sentences.shape[0],100))\n",
    "word2vec_arr2 = np.zeros((sentences.shape[0],100))\n",
    "word2vec_arr3 = np.zeros((sentences.shape[0],100))\n",
    "word2vec_arr4 = np.zeros((sentences.shape[0],200))\n",
    "word2vec_arr5 = np.zeros((sentences.shape[0],200))\n",
    "word2vec_arr6 = np.zeros((sentences.shape[0],200))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    word2vec_arr1[i,:] = np.mean([model1[lemma] for lemma in sentence], axis=0)\n",
    "    word2vec_arr2[i,:] = np.mean([model2[lemma] for lemma in sentence], axis=0)\n",
    "    word2vec_arr3[i,:] = np.mean([model3[lemma] for lemma in sentence], axis=0)\n",
    "    word2vec_arr4[i,:] = np.mean([model4[lemma] for lemma in sentence], axis=0)\n",
    "    word2vec_arr5[i,:] = np.mean([model5[lemma] for lemma in sentence], axis=0)\n",
    "    word2vec_arr6[i,:] = np.mean([model6[lemma] for lemma in sentence], axis=0)\n",
    "\n",
    "word2vec_arr1 = pd.DataFrame(word2vec_arr1)\n",
    "word2vec_arr2 = pd.DataFrame(word2vec_arr2)\n",
    "word2vec_arr3 = pd.DataFrame(word2vec_arr3)\n",
    "word2vec_arr4 = pd.DataFrame(word2vec_arr4)\n",
    "word2vec_arr5 = pd.DataFrame(word2vec_arr5)\n",
    "word2vec_arr6 = pd.DataFrame(word2vec_arr6)\n",
    "\n",
    "sentences1 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr1], axis=1)\n",
    "sentences1.dropna(inplace=True)\n",
    "\n",
    "sentences2 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr2], axis=1)\n",
    "sentences2.dropna(inplace=True)\n",
    "\n",
    "sentences3 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr3], axis=1)\n",
    "sentences3.dropna(inplace=True)\n",
    "\n",
    "sentences4 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr4], axis=1)\n",
    "sentences4.dropna(inplace=True)\n",
    "\n",
    "sentences5 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr5], axis=1)\n",
    "sentences5.dropna(inplace=True)\n",
    "\n",
    "sentences6 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr6], axis=1)\n",
    "sentences6.dropna(inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
